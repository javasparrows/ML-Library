{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"../../../../datasets\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/yuki/Work/ML-Library/datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef452e87f9d4297993bc36b0ce04d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/yuki/Work/ML-Library/datasets/cifar-10-python.tar.gz to /home/yuki/Work/ML-Library/datasets\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 17:10:37,537\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py:610: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-05 17:10:38 (running for 00:00:00.17)\n",
      "Memory usage on this node: 5.4/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 |\n",
      "| train_cifar_70663_00001 | PENDING  |                     |            2 |  128 |    4 | 0.00952107  |\n",
      "| train_cifar_70663_00002 | PENDING  |                     |            4 |    4 |   64 | 0.00023588  |\n",
      "| train_cifar_70663_00003 | PENDING  |                     |            2 |  256 |  128 | 0.00374915  |\n",
      "| train_cifar_70663_00004 | PENDING  |                     |            2 |    4 |   32 | 0.00925498  |\n",
      "| train_cifar_70663_00005 | PENDING  |                     |           16 |    8 |   16 | 0.0914261   |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492040)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492040)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m Files already downloaded and verified\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:10:45 (running for 00:00:06.83)\n",
      "Memory usage on this node: 7.8/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:10:50 (running for 00:00:11.84)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m [1,  2000] loss: 2.303\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1,  2000] loss: 2.244\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [1,  2000] loss: 2.304\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1,  2000] loss: 2.297\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1,  2000] loss: 2.132\n",
      "\u001b[2m\u001b[36m(func pid=492040)\u001b[0m [1,  2000] loss: 2.308\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:10:55 (running for 00:00:16.85)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1,  4000] loss: 1.170\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [1,  4000] loss: 1.126\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1,  4000] loss: 1.118\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m [1,  4000] loss: 1.144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  accuracy</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">   loss</th><th>node_ip     </th><th style=\"text-align: right;\">   pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_70663_00000</td><td style=\"text-align: right;\">    0.3509</td><td>2023-01-05_17-11-30</td><td>False </td><td>                </td><td>6c358f095075497a969b1706ef562511</td><td>ubuntu-20 </td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">1.73648</td><td>192.168.1.42</td><td style=\"text-align: right;\">491963</td><td>True               </td><td style=\"text-align: right;\">             49.9953</td><td style=\"text-align: right;\">           24.3407</td><td style=\"text-align: right;\">       49.9953</td><td style=\"text-align: right;\"> 1672906290</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">70663_00000</td><td style=\"text-align: right;\">   0.00231314</td></tr>\n",
       "<tr><td>train_cifar_70663_00002</td><td style=\"text-align: right;\">    0.3206</td><td>2023-01-05_17-11-25</td><td>False </td><td>                </td><td>7b248aab4ef94e8cac128bc1dbbe2f9b</td><td>ubuntu-20 </td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.82446</td><td>192.168.1.42</td><td style=\"text-align: right;\">492034</td><td>True               </td><td style=\"text-align: right;\">             42.6982</td><td style=\"text-align: right;\">           42.6982</td><td style=\"text-align: right;\">       42.6982</td><td style=\"text-align: right;\"> 1672906285</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">70663_00002</td><td style=\"text-align: right;\">   0.00505352</td></tr>\n",
       "<tr><td>train_cifar_70663_00005</td><td style=\"text-align: right;\">    0.0985</td><td>2023-01-05_17-11-30</td><td>False </td><td>                </td><td>227cdf3f99944ba89ef390d37d8afba9</td><td>ubuntu-20 </td><td style=\"text-align: right;\">                         3</td><td style=\"text-align: right;\">2.31853</td><td>192.168.1.42</td><td style=\"text-align: right;\">492040</td><td>True               </td><td style=\"text-align: right;\">             48.2714</td><td style=\"text-align: right;\">           15.6815</td><td style=\"text-align: right;\">       48.2714</td><td style=\"text-align: right;\"> 1672906290</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">70663_00005</td><td style=\"text-align: right;\">   0.0218329 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1,  4000] loss: 0.972\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1,  6000] loss: 0.771\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:04 (running for 00:00:25.94)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.3288042736053467\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |   loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+--------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 |        |            |                      |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |        |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |        |            |                      |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |        |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |        |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.3288 |     0.0983 |                    1 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |        |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |        |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |        |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |        |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+--------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [1,  6000] loss: 0.692\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1,  6000] loss: 0.771\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1,  6000] loss: 0.613\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1,  8000] loss: 0.578\n",
      "\u001b[2m\u001b[36m(func pid=492040)\u001b[0m [2,  2000] loss: 2.315\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:11 (running for 00:00:32.48)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -2.258656000947952\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 2.18851 |     0.1918 |                    1 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |         |            |                      |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.3288  |     0.0983 |                    1 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [1,  8000] loss: 0.488\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m [2,  2000] loss: 2.053\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1,  8000] loss: 0.578\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1,  8000] loss: 0.459\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1, 10000] loss: 0.462\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [1, 10000] loss: 0.370\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:20 (running for 00:00:41.61)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.315613168334961 | Iter 1.000: -2.258656000947952\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 2.18851 |     0.1918 |                    1 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |         |            |                      |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.31561 |     0.0939 |                    2 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=491963)\u001b[0m [2,  4000] loss: 0.933\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1, 10000] loss: 0.463\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1, 10000] loss: 0.352\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1, 12000] loss: 0.386\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:25 (running for 00:00:46.62)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.315613168334961 | Iter 1.000: -2.258656000947952\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 2.18851 |     0.1918 |                    1 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  |         |            |                      |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.31561 |     0.0939 |                    2 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=492040)\u001b[0m [3,  2000] loss: 2.315\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:30 (running for 00:00:51.66)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.315613168334961 | Iter 1.000: -2.188507728290558\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 2.18851 |     0.1918 |                    1 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  | 1.82446 |     0.3206 |                    1 |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.31561 |     0.0939 |                    2 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(func pid=492038)\u001b[0m [1, 14000] loss: 0.330\n",
      "\u001b[2m\u001b[36m(func pid=492036)\u001b[0m [1, 12000] loss: 0.295\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m [1, 12000] loss: 0.386\n",
      "\u001b[2m\u001b[36m(func pid=492034)\u001b[0m [2,  2000] loss: 1.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 17:11:33,817\tWARNING tune.py:690 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:35 (running for 00:00:57.30)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.026046961688995 | Iter 1.000: -2.188507728290558\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 1.73648 |     0.3509 |                    2 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  | 1.82446 |     0.3206 |                    1 |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.31853 |     0.0985 |                    3 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-01-05 17:11:35 (running for 00:00:57.31)\n",
      "Memory usage on this node: 7.9/47.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -2.026046961688995 | Iter 1.000: -2.188507728290558\n",
      "Resources requested: 12.0/12 CPUs, 0/1 GPUs, 0.0/25.26 GiB heap, 0.0/12.63 GiB objects\n",
      "Result logdir: /home/yuki/ray_results/train_cifar_2023-01-05_17-10-38\n",
      "Number of trials: 10/10 (4 PENDING, 6 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n",
      "|-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------|\n",
      "| train_cifar_70663_00000 | RUNNING  | 192.168.1.42:491963 |            8 |   16 |   32 | 0.000267612 | 1.73648 |     0.3509 |                    2 |\n",
      "| train_cifar_70663_00001 | RUNNING  | 192.168.1.42:492032 |            2 |  128 |    4 | 0.00952107  |         |            |                      |\n",
      "| train_cifar_70663_00002 | RUNNING  | 192.168.1.42:492034 |            4 |    4 |   64 | 0.00023588  | 1.82446 |     0.3206 |                    1 |\n",
      "| train_cifar_70663_00003 | RUNNING  | 192.168.1.42:492036 |            2 |  256 |  128 | 0.00374915  |         |            |                      |\n",
      "| train_cifar_70663_00004 | RUNNING  | 192.168.1.42:492038 |            2 |    4 |   32 | 0.00925498  |         |            |                      |\n",
      "| train_cifar_70663_00005 | RUNNING  | 192.168.1.42:492040 |           16 |    8 |   16 | 0.0914261   | 2.31853 |     0.0985 |                    3 |\n",
      "| train_cifar_70663_00006 | PENDING  |                     |            4 |    8 |    8 | 0.0160413   |         |            |                      |\n",
      "| train_cifar_70663_00007 | PENDING  |                     |           16 |  128 |  128 | 0.00567286  |         |            |                      |\n",
      "| train_cifar_70663_00008 | PENDING  |                     |            8 |    8 |    4 | 0.0029733   |         |            |                      |\n",
      "| train_cifar_70663_00009 | PENDING  |                     |            2 |    4 |   32 | 0.0699086   |         |            |                      |\n",
      "+-------------------------+----------+---------------------+--------------+------+------+-------------+---------+------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/resource_sharer.py\", line 138, in _serve\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     with self._listener.accept() as conn:\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/connection.py\", line 471, in accept\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     answer_challenge(c, self._authkey)\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/connection.py\", line 762, in answer_challenge\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     response = connection.recv_bytes(256)        # reject large message\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     buf = self._recv_bytes(maxlength)\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m   File \"/home/yuki/.pyenv/versions/3.10.7/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m     chunk = read(handle, remaining)\n",
      "\u001b[2m\u001b[36m(func pid=492032)\u001b[0m ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2023-01-05 17:11:36,062\tERROR tune.py:758 -- Trials did not complete: [train_cifar_70663_00000, train_cifar_70663_00001, train_cifar_70663_00002, train_cifar_70663_00003, train_cifar_70663_00004, train_cifar_70663_00005, train_cifar_70663_00006, train_cifar_70663_00007, train_cifar_70663_00008, train_cifar_70663_00009]\n",
      "2023-01-05 17:11:36,063\tINFO tune.py:762 -- Total run time: 57.60 seconds (57.30 seconds for the tuning loop).\n",
      "2023-01-05 17:11:36,064\tWARNING tune.py:768 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 16, 'l2': 32, 'lr': 0.000267612331002683, 'batch_size': 8}\n",
      "Best trial final validation loss: 1.7364807550430297\n",
      "Best trial final validation accuracy: 0.3509\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_TrackedCheckpoint' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial test set accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_acc))\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[39m# You can change the number of GPUs per trial here:\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     main(num_samples\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, max_num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, gpus_per_trial\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m         best_trained_model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDataParallel(best_trained_model)\n\u001b[1;32m     40\u001b[0m best_trained_model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 42\u001b[0m best_checkpoint_dir \u001b[39m=\u001b[39m best_trial\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mvalue\n\u001b[1;32m     43\u001b[0m model_state, optimizer_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     44\u001b[0m     best_checkpoint_dir, \u001b[39m\"\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     45\u001b[0m best_trained_model\u001b[39m.\u001b[39mload_state_dict(model_state)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_TrackedCheckpoint' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    data_dir = os.path.abspath(\"../../../../datasets\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c2760a9c2997f9bc5632bfe1b04c6e39ec7256b7fa1e795474f5afcfd79ce3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
