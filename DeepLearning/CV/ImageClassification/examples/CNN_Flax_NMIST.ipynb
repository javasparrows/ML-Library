{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flaxの[Getting Started](https://flax.readthedocs.io/en/latest/getting_started.html)を参照"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This tutorial demonstrates how to construct a simple convolutional neural network (CNN) using the [Flax](https://flax.readthedocs.io/en/latest/) Linen API and train the network for image classification on the MNIST dataset.\n",
    "\n",
    "Note: This notebook is based on Flax’s official [MNIST Example](https://github.com/google/flax/tree/main/examples/mnist). If you see any changes between the two feel free to create a [pull request](https://github.com/google/flax/compare) to synchronize this Colab with the actual example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports\n",
    "\n",
    "Import JAX, [JAX NumPy](https://jax.readthedocs.io/en/latest/jax.numpy.html), Flax, ordinary NumPy, and TensorFlow Datasets (TFDS). Flax can use any data-loading pipeline and this example demonstrates how to utilize TFDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_strict_conv_algorithm_picker=false\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_autotune_level=0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".5\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # Optimizers\n",
    "import tensorflow_datasets as tfds     # TFDS for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define network\n",
    "\n",
    "Create a convolutional neural network with the Linen API by subclassing [Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#core-module-abstraction). Because the architecture in this example is relatively simple—you’re just stacking layers—you can define the inlined submodules directly within the `__call__` method and wrap it with the [@compact](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#compact-methods) decorator. To learn more about the Flax Linen @compact decorator, refer to the [setup vs compact](https://flax.readthedocs.io/en/latest/guides/setup_or_nncompact.html) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define loss\n",
    "\n",
    "We simply use `optax.softmax_cross_entropy()`. Note that this function expects both `logits` and `labels` to have shape `[batch, num_classes]`. Since the labels will be read from TFDS as integer values, we first need to convert them to a onehot encoding.\n",
    "\n",
    "Our function returns a simple scalar value ready for optimization, so we first take the mean of the vector shaped `[batch]` returned by Optax’s loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, logits, labels):\n",
    "  labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "  return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metric computation\n",
    "\n",
    "For loss and accuracy metrics, create a separate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading data\n",
    "\n",
    "Define a function that loads and prepares the MNIST dataset and converts the samples to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "  \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "  ds_builder = tfds.builder('mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create train state\n",
    "\n",
    "A common pattern in Flax is to create a single dataclass that represents the entire training state, including step number, parameters, and optimizer state.\n",
    "\n",
    "Also adding optimizer & model to this state has the advantage that we only need to pass around a single argument to functions like `train_step()` (see below).\n",
    "\n",
    "Because this is such a common pattern, Flax provides the class [flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#compact-methods) that serves most basic usecases. Usually one would subclass it to add more data to be tracked, but in this example we can use it without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN()\n",
    "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training step\n",
    "\n",
    "A function that:\n",
    "\n",
    "- Evaluates the neural network given the parameters and a batch of input images with the [Module.apply](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.apply) method (forward pass).\n",
    "- Computes the `cross_entropy_loss` loss function.\n",
    "- Evaluates the gradient of the loss function using [jax.grad](https://jax.readthedocs.io/en/latest/jax.html#jax.grad).\n",
    "- Applies a [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions) of gradients to the optimizer to update the model’s parameters.\n",
    "- Computes the metrics using `compute_metrics` (defined earlier).\n",
    "\n",
    "Use JAX’s [@jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit) decorator to trace the entire `train_step` function and just-in-time compile it with [XLA](https://www.tensorflow.org/xla) into fused device operations that run faster and more efficiently on hardware accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "    return loss, logits\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, logits = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "  return state, metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation step\n",
    "Create a function that evaluates your model on the test set with [Module.apply](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "  logits = CNN().apply({'params': params}, batch['image'])\n",
    "  return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train function\n",
    "\n",
    "Define a training function that:\n",
    "\n",
    "- Shuffles the training data before each epoch using [jax.random.permutation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.permutation.html) that takes a PRNGKey as a parameter (check the [JAX - the sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#JAX-PRNG)).\n",
    "- Runs an optimization step for each batch.\n",
    "- Asynchronously retrieves the training metrics from the device with `jax.device_get` and computes their mean across each batch in an epoch.\n",
    "- Returns the optimizer with updated parameters and the training loss and accuracy metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "  \"\"\"Train for a single epoch.\"\"\"\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, train_ds_size) # get a randomized index array\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size)) # index array, where each row is a batch\n",
    "  batch_metrics = []\n",
    "  for perm in perms:\n",
    "    batch = {k: v[perm, ...] for k, v in train_ds.items()} # dict{'image': array, 'label': array}\n",
    "    state, metrics = train_step(state, batch)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "\n",
    "  print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
    "      epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "  return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Eval function\n",
    "\n",
    "Create a model evaluation function that:\n",
    "\n",
    "- Retrieves the evaluation metrics from the device with `jax.device_get`.\n",
    "- Copies the metrics [data stored](https://flax.readthedocs.io/en/latest/advanced_topics/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables) in a JAX [pytree](https://jax.readthedocs.io/en/latest/pytrees.html#pytrees-and-jax-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "  metrics = eval_step(params, test_ds)\n",
    "  metrics = jax.device_get(metrics)\n",
    "  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics) # map the function over all leaves in metrics\n",
    "  return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 13:41:08.697161: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Seed randomness\n",
    "\n",
    "- Get one [PRNGKey](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html#jax.random.PRNGKey) and [split](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax.random.split) it to get a second key that you’ll use for parameter initialization. (Learn more about [PRNG chains](https://flax.readthedocs.io/en/latest/advanced_topics/linen_design_principles.html#how-are-parameters-represented-and-how-do-we-handle-general-differentiable-algorithms-that-update-stateful-variables) and [JAX PRNG design](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Initialize train state\n",
    "\n",
    "Remember that function initializes both the model parameters and the optimizer and puts both into the training state dataclass that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Train and evaluate\n",
    "\n",
    "Once the training and testing is done after 10 epochs, the output should show that your model was able to achieve approximately 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.1405, accuracy: 95.81\n",
      " test epoch: 1, loss: 0.05, accuracy: 98.22\n",
      "Epoch 1 in 21.68 sec\n",
      "train epoch: 2, loss: 0.0502, accuracy: 98.44\n",
      " test epoch: 2, loss: 0.04, accuracy: 98.66\n",
      "Epoch 2 in 20.89 sec\n",
      "train epoch: 3, loss: 0.0343, accuracy: 98.94\n",
      " test epoch: 3, loss: 0.03, accuracy: 98.97\n",
      "Epoch 3 in 21.48 sec\n",
      "train epoch: 4, loss: 0.0259, accuracy: 99.16\n",
      " test epoch: 4, loss: 0.04, accuracy: 99.00\n",
      "Epoch 4 in 21.08 sec\n",
      "train epoch: 5, loss: 0.0198, accuracy: 99.42\n",
      " test epoch: 5, loss: 0.04, accuracy: 98.93\n",
      "Epoch 5 in 20.91 sec\n",
      "train epoch: 6, loss: 0.0180, accuracy: 99.42\n",
      " test epoch: 6, loss: 0.05, accuracy: 98.48\n",
      "Epoch 6 in 21.08 sec\n",
      "train epoch: 7, loss: 0.0153, accuracy: 99.53\n",
      " test epoch: 7, loss: 0.05, accuracy: 98.60\n",
      "Epoch 7 in 20.78 sec\n",
      "train epoch: 8, loss: 0.0133, accuracy: 99.59\n",
      " test epoch: 8, loss: 0.05, accuracy: 98.93\n",
      "Epoch 8 in 21.16 sec\n",
      "train epoch: 9, loss: 0.0108, accuracy: 99.65\n",
      " test epoch: 9, loss: 0.05, accuracy: 99.00\n",
      "Epoch 9 in 20.90 sec\n",
      "train epoch: 10, loss: 0.0112, accuracy: 99.66\n",
      " test epoch: 10, loss: 0.04, accuracy: 99.16\n",
      "Epoch 10 in 21.06 sec\n",
      "Total in 218.65 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  start_epoch_time = time.time()\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "  # Run an optimization step over a training batch\n",
    "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "  epoch_time = time.time() - start_epoch_time\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "  print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, test_loss, test_accuracy * 100))\n",
    "  print('Epoch %d in %.2f sec' % (epoch, epoch_time))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('Total in %.2f sec' % (total_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You made it to the end of the annotated MNIST example. You can revisit the same example, but structured differently as a couple of Python modules, test modules, config files, another Colab, and documentation in Flax’s Git repo: [https://github.com/google/flax/tree/main/examples/mnist](https://github.com/google/flax/tree/main/examples/mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6ce19cd792589e8ccf8babb3986c3b6ee32845423f27865ddfac7006bec93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
