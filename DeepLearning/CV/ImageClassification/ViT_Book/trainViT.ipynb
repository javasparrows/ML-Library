{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from vit import Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 10\n",
    "# batch_size, channel, height, width = 2, 3, 32, 32\n",
    "# x = torch.randn(batch_size, channel, height, width)\n",
    "# vit = Vit(in_channels=channel, num_classes=num_classes)\n",
    "# pred = vit(x)\n",
    "\n",
    "# # (2, 10)(=(B, M))になっていることを確認\n",
    "# print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "path = '/Users/yukik/Work/ML/ML-Library/datasets/ImageNet/ILSVRC/Data/CLS-LOC/train/n01440764/n01440764_18.JPEG'\n",
    "img = read_image(path=path)\n",
    "# img = T.ToPILImage(img)\n",
    "img = img.to(torch.float32)\n",
    "print(type(img))\n",
    "# T.ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, mode='train'):\n",
    "        self.df = pd.read_csv(annotations_file)\n",
    "        self.labelNums = np.array(self.getLabels()[0]).astype(np.uint8)\n",
    "        self.labelNames = self.getLabels()[1]\n",
    "        self.img_names = self.getImgNames()\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labelNums)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labelNum = self.labelNums[idx]\n",
    "        labelName = self.labelNames[idx]\n",
    "        if self.mode == 'train':\n",
    "            img_path = os.path.join(self.img_dir, labelName, self.img_names[idx]) + '.JPEG'\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_dir, self.img_names[idx]) + '.JPEG'\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            labelNum = self.target_transform(labelNum)\n",
    "        return img, labelNum\n",
    "    \n",
    "    def getLabels(self):\n",
    "        labelNames = [row.split(' ')[0] for row in self.df['PredictionString'].values.tolist()]\n",
    "        le = LabelEncoder()\n",
    "        labelNums = le.fit_transform(labelNames).tolist()\n",
    "        lableMap = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "        return labelNums, labelNames, lableMap\n",
    "    \n",
    "    def getImgNames(self):\n",
    "        return self.df['ImageId'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# img = train_dataset[0][0]\n",
    "# img = np.array(img).transpose(1, 2, 0) #.astype(np.uint8)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomResizedCrop(img_size),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "train_dataset = ImageNetDataset(\n",
    "    annotations_file = '../../../../datasets/ImageNet/LOC_train_solution.csv',\n",
    "    img_dir = '../../../../datasets/ImageNet/ILSVRC/Data/CLS-LOC/train',\n",
    "    transform = train_transform,\n",
    "    mode = 'train'\n",
    "    )\n",
    "\n",
    "valid_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize([img_size, img_size]),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "valid_dataset = ImageNetDataset(\n",
    "    annotations_file = '../../../../datasets/ImageNet/LOC_val_solution.csv',\n",
    "    img_dir = '../../../../datasets/ImageNet/ILSVRC/Data/CLS-LOC/val',\n",
    "    transform = valid_transform,\n",
    "    mode = 'val'\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True,\n",
    "    num_workers=0, pin_memory=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=128, shuffle=False,\n",
    "    num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(train_dataset.getLabels()[0]))  # 1000\n",
    "channel, height, width = 3, img_size, img_size\n",
    "model = Vit(in_channels=channel,\n",
    "            num_classes=num_classes,\n",
    "            image_size=img_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "results = []\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_total = 0\n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.argmax(axis=1) == labels).sum().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "    # if i % printEpochs == (printEpochs - 1):    # print every 10 mini-batches\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valid_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            val_acc += (outputs.argmax(axis=1) == labels).sum().item()\n",
    "            val_total += len(labels)\n",
    "    \n",
    "    results.append([epoch+1, train_loss/train_total, val_loss/val_total, train_acc/train_total, val_acc/val_total])\n",
    "    print('[{}/{}] train loss {:.4f} val loss {:.4f} | train acc {:.4f} val acc {:.4f}'.format(\n",
    "        epoch+1, EPOCHS, results[1], results[2], results[3], results[4]\n",
    "    ))\n",
    "\n",
    "print('Finished Training')\n",
    "df = pd.DataFrame(data=results, columns=['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "df.to_csv('ViT_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6ce19cd792589e8ccf8babb3986c3b6ee32845423f27865ddfac7006bec93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
