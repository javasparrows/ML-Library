{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMlp(nn.Module):\n",
    "  def __init__(self,\n",
    "               vec_length:int=16,\n",
    "               hidden_unit_1:int=8,\n",
    "               hidden_unit_2:int=2):\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      vec_length: 入力ベクトルの長さ\n",
    "      hidden_unit_1: 1つ目の線形層のニューロン数\n",
    "      hidden_unit_2: 2つ目の線形層のニューロン数\n",
    "    \"\"\"\n",
    "    # 継承しているnn.Moduleの__init__()メソッドの呼び出し\n",
    "    super(SimpleMlp, self).__init__()\n",
    "    \n",
    "    # 1つ目の線形層\n",
    "    self.layer1 = nn.Linear(vec_length, hidden_unit_1)\n",
    "    # 活性化関数のReLU\n",
    "    self.relu = nn.ReLU()\n",
    "    # 2つ目の線形層\n",
    "    self.layer2 = nn.Linear(hidden_unit_1, hidden_unit_2)\n",
    "  \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"準伝搬は、線形層-->ReLU-->線形層の順番\n",
    "    引数:\n",
    "      x: 入力 (B, D_in)\n",
    "        B: バッチサイズ, D_in: ベクトルの長さ\n",
    "    返り値:\n",
    "      out: 入力 (B, D_in)\n",
    "        B: バッチサイズ, D_in: ベクトルの長さ\n",
    "    \"\"\"\n",
    "    # 1つ目の線形層\n",
    "    out = self.layer1(x)\n",
    "    # ReLU\n",
    "    out = self.relu(out)\n",
    "    # 2つ目の線形層\n",
    "    out = self.layer2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "vec_length = 16  # 入力ベクトルの長さ\n",
    "hidden_unit_1 = 8  # 1つ目の線形層のニューロン数\n",
    "hidden_unit_2 = 2  # 2つ目の線形層のニューロン数\n",
    "\n",
    "batch_size = 4  # バッチサイズ。入力ベクトルの数\n",
    "\n",
    "# 入力ベクトル。xの形像: (4, 16)\n",
    "x = torch.randn(batch_size, vec_length)\n",
    "# MLPを定義\n",
    "net = SimpleMlp(vec_length, hidden_unit_1, hidden_unit_2)\n",
    "# MLPで準伝搬\n",
    "out = net(x)\n",
    "# MLPの出力outの形状が(4, 2)であることを確認\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VitInputLayer(nn.Module):\n",
    "  def __init__(self,\n",
    "               in_channels:int=3,\n",
    "               emb_dim:int=384,\n",
    "               num_patch_row:int=2,\n",
    "               image_size:int=32\n",
    "               ):\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      in_channels: 入力画像のチャンネル数\n",
    "      emb_dim: 埋め込み後のベクトルの長さ\n",
    "      num_patch_row: 高さ方向のバッチ数。例は2x2であるため、2をデフォルト値としている\n",
    "      image_size: 入力画像の1辺の大きさ。入力画像の高さと幅は同じであると設定\n",
    "    \"\"\"\n",
    "    super(VitInputLayer, self).__init__()\n",
    "    self.in_channels=in_channels\n",
    "    self.emb_dim=emb_dim\n",
    "    self.num_patch_row=num_patch_row\n",
    "    self.image_size=image_size\n",
    "    \n",
    "    # パッチの数\n",
    "    ## 例：入力画像を2x2のバッチに分ける場合、num_patchは4\n",
    "    self.num_patch = self.num_patch_row**2\n",
    "    \n",
    "    # パッチの大きさ\n",
    "    ## 例：入力画像の1辺の大きさが32の場合、patch_sizeは16\n",
    "    self.patch_size = int(self.image_size // self.num_patch_row)\n",
    "    \n",
    "    # 入力画像のパッチへの分割 & パッチの埋め込みを一気に行う層\n",
    "    self.patch_emb_layer = nn.Conv2d(\n",
    "      in_channels=self.in_channels,\n",
    "      out_channels=self.emb_dim,\n",
    "      kernel_size=self.patch_size,\n",
    "      stride=self.patch_size\n",
    "    )\n",
    "    \n",
    "    # クラストークン\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "    \n",
    "    # 位置埋め込み\n",
    "    ## クラストークンが先頭に結合されているため、\n",
    "    ## 長さemb_dimの位置埋め込みベクトルを(バッチ数+1)個用意\n",
    "    self.pos_emb = nn.Parameter(torch.randn(1, self.num_patch+1, emb_dim))\n",
    "  \n",
    "  \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      x: 入力画像。形状は (B, C, H, W)\n",
    "        B: バッチサイズ, C: チャンネル数, H: 高さ, W: 幅\n",
    "    返り値:\n",
    "      z_0: ViTへの入力。形状は (B, N, D)\n",
    "        B: バッチサイズ, N: トークン数, D: 埋め込みベクトルの長さ\n",
    "    \"\"\"\n",
    "    \n",
    "    # パッチの埋め込み $ flatten\n",
    "    ## パッチの埋め込み (B, C, H, W) --> (B, D, H/P, W/P)\n",
    "    ## ここで、Pはパッチ1辺の長さ\n",
    "    z_0 = self.patch_emb_layer(x)\n",
    "    \n",
    "    ## パッチのflatten (B, D, H/P, W/P) --> (B, D, Np)\n",
    "    ## ここで、Npはパッチの数 (=H*W/P^2)\n",
    "    z_0 = z_0.flatten(2)\n",
    "    \n",
    "    ## 軸の入れ替え (B, D, Np) --> (B, Np, D)\n",
    "    z_0 = z_0.transpose(1, 2)\n",
    "    \n",
    "    # パッチの埋め込みの先頭にクラストークンを結合\n",
    "    ## (B, Np, D) --> (B, N, D)\n",
    "    ## N = (Np + 1)であることに留意\n",
    "    ## また、cls_tokenの形状は(1,1,D)であるため、\n",
    "    ## repeatメソッドによって(B,1,D)に変換してからパッチの埋め込みとの結合を行う\n",
    "    z_0 = torch.cat(\n",
    "      [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
    "    \n",
    "    # 位置埋め込みの加算\n",
    "    ## (B, N, D) --> (B, N, D)\n",
    "    z_0 = z_0 + self.pos_emb\n",
    "    \n",
    "    return z_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装したVitInputLayerへの入力が正常に出力されるかを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size, channel, height, width = 2, 3, 32, 32\n",
    "x = torch.randn(batch_size, channel, height, width)\n",
    "input_layer = VitInputLayer(num_patch_row=2)\n",
    "z_0=input_layer(x)\n",
    "\n",
    "# (2, 5, 384)(=(B, N, D))になっていることを確認\n",
    "print(z_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attentionの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "  def __init__(self,\n",
    "               emb_dim:int=384,\n",
    "               head:int=3,\n",
    "               dropout:float=0\n",
    "               ) -> None:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      emb_dim: 埋め込み後のベクトルの長さ\n",
    "      head: ヘッドの数\n",
    "      dropout: ドロップアウト率\n",
    "    \"\"\"\n",
    "    super(MultiHeadSelfAttention, self).__init__()\n",
    "    self.head = head\n",
    "    self.emb_dim = emb_dim\n",
    "    self.head_dim = emb_dim // head\n",
    "    self.sqrt_dh = self.head_dim**0.5  # D_hの二乗根。qk^Tを割るための係数\n",
    "    \n",
    "    # 入力をq,k,vに埋め込むための線形層\n",
    "    self.w_q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "    self.w_k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "    self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "    \n",
    "    # 実装ではドロップアウト層も用いる\n",
    "    self.attn_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    # MHSAの結果を出力に埋め込むための線形層\n",
    "    ## 実装ではドロップアウト層も用いる\n",
    "    self.w_o = nn.Sequential(\n",
    "      nn.Linear(emb_dim, emb_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "  \n",
    "  def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      z: MHSAへの入力。形状は(B, N, D)\n",
    "        B:バッチサイズ, N:トークンの数, D:ベクトルの長さ\n",
    "    \n",
    "    返り値:\n",
    "      out: MHSAの出力。形状は(B, N, D)\n",
    "        B:バッチサイズ, N:トークンの数, D:埋め込みベクトルの長さ\n",
    "    \"\"\"\n",
    "    batch_size, num_patch, _ = z.size()\n",
    "    \n",
    "    # 埋め込み\n",
    "    ## (B, N, D) --> (B, N, D)\n",
    "    q = self.w_q(z)\n",
    "    k = self.w_k(z)\n",
    "    v = self.w_v(z)\n",
    "    \n",
    "    # q, k, vをヘッドに分ける\n",
    "    ## まずベクトルをヘッドの個数(h)に分ける\n",
    "    ## (B, N, D) --> (B, N, h, D/h)\n",
    "    q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "    k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "    v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "    ## Self-Attentionができるように、\n",
    "    ## （バッチサイズ、ヘッド、トークン数、バッチのベクトル）の形に変更する\n",
    "    ## (B, N, h, D/h) --> (B, h, N, D/h)\n",
    "    q = q.transpose(1,2)\n",
    "    k = k.transpose(1,2)\n",
    "    v = v.transpose(1,2)\n",
    "    \n",
    "    # 内積\n",
    "    ## (B, h, N, D/h) --> (B, h, D/h, N)\n",
    "    k_T = k.transpose(2, 3)\n",
    "    ## (B, h, N, D/h) x (B, h, D/h, N) --> (B, h, N, N)\n",
    "    dots = (q @ k_T) / self.sqrt_dh\n",
    "    ## 列方向にソフトマックス関数\n",
    "    attn = F.softmax(dots, dim=-1)\n",
    "    ## ドロップアウト\n",
    "    attn = self.attn_drop(attn)\n",
    "    \n",
    "    # 加重和\n",
    "    ## (B, h, N, N) x (B, h, N, D/h) --> (B, h, N, D/h)\n",
    "    out = attn @ v\n",
    "    ## (B, h, N, D/h) --> (B, N, h, D/h)\n",
    "    out = out.transpose(1, 2)\n",
    "    ## (B, N, h, D/h) --> (B, N, D)\n",
    "    out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
    "    \n",
    "    # 出力層\n",
    "    ## (B, N, D) --> (B, N, D)\n",
    "    out = self.w_o(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "mhsa = MultiHeadSelfAttention()\n",
    "out = mhsa(z_0)  # z_0はz_0=input_layer(x)で、形状は(B, N, D)\n",
    "\n",
    "# (2, 5, 384)(=(B, N, D))になっていることを確認　\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VitEncoderBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    emb_dim:int=384,\n",
    "    head:int=8,\n",
    "    hidden_dim:int=384*4,\n",
    "    dropout:float=0.\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      emb_dim: 埋め込み後のベクトルの長さ\n",
    "      head: ヘッドの数\n",
    "      hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ\n",
    "      　原論文に従ってemb_dimの4倍をデフォルト値としている\n",
    "      dropout: ドロップアウト率\n",
    "    \"\"\"\n",
    "    super(VitEncoderBlock, self).__init__()\n",
    "    # 1つ目のLayer Normalization\n",
    "    self.ln1 = nn.LayerNorm(emb_dim)\n",
    "    # MHSA\n",
    "    self.msa = MultiHeadSelfAttention(\n",
    "      emb_dim=emb_dim,\n",
    "      head=head,\n",
    "      dropout=dropout,\n",
    "    )\n",
    "    # 2つ目のLayer Normalization\n",
    "    self.ln2 = nn.LayerNorm(emb_dim)\n",
    "    # MLP\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(emb_dim, hidden_dim),\n",
    "      nn.GELU(),\n",
    "      nn.Dropout(dropout),\n",
    "      nn.Linear(hidden_dim, emb_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "  \n",
    "  def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      z: Encoder Blockへの入力。形状は(B, N, D)\n",
    "        B: バッチサイズ, N: トークンの数, D: ベクトルの長さ\n",
    "    \n",
    "    返り値:\n",
    "      out: Encoder Blockへの出力。形状は(B, N, D)\n",
    "        B: バッチサイズ, N: トークンの数, D: 埋め込みベクトルの長さ\n",
    "    \"\"\"\n",
    "    out = self.msa(self.ln1(z)) + z\n",
    "    # Encoder Blockの後半部分\n",
    "    out = self.mlp(self.ln2(out)) + out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "vit_enc = VitEncoderBlock()\n",
    "z_1 = vit_enc(z_0)  # z_0はz_0=input_layer(x)で、形状は(B, N, D)\n",
    "\n",
    "# (2, 5, 384)(=(B, N, D))になっていることを確認\n",
    "print(z_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Vit(nn.Module):\n",
    "  def __init__(self,\n",
    "    in_channels:int=3,\n",
    "    num_classes:int=10,\n",
    "    emb_dim:int=384,\n",
    "    num_patch_row:int=2,\n",
    "    image_size:int=32,\n",
    "    num_blocks:int=7,\n",
    "    head:int=8,\n",
    "    hidden_dim:int=384*4,\n",
    "    dropout:float=0.\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      in_channels: 入力画像のチャンネル数\n",
    "      num_classes: 画像分類のクラス数\n",
    "      emb_dim: 埋め込み後のベクトルの長さ\n",
    "      num_patch_row: 1辺のパッチの数\n",
    "      image_size: 入力画像の1辺の大きさ。入力画像の高さと幅は同じであると仮定\n",
    "      num_blocks: Encoder Blockの数\n",
    "      head: ヘッドの数\n",
    "      hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ\n",
    "      dropout: ドロップアウト率\n",
    "    \"\"\"\n",
    "    super(Vit, self).__init__()\n",
    "    \n",
    "    # Input Layer\n",
    "    self.input_layer = VitInputLayer(\n",
    "      in_channels,\n",
    "      emb_dim,\n",
    "      num_patch_row,\n",
    "      image_size)\n",
    "    \n",
    "    # Encoder. Encoder Blockの多段\n",
    "    self.encoder = nn.Sequential(*[\n",
    "      VitEncoderBlock(\n",
    "        emb_dim=emb_dim,\n",
    "        head=head,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout\n",
    "      )\n",
    "      for _ in range(num_blocks)])\n",
    "    \n",
    "    # MLP Head\n",
    "    self.mlp_head = nn.Sequential(\n",
    "      nn.LayerNorm(emb_dim),\n",
    "      nn.Linear(emb_dim, num_classes)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    引数:\n",
    "      x: ViTへの入力画像。形状は(B, C, H, W)\n",
    "        B:バッチサイズ, C:チャンネル数, H:高さ, W:幅\n",
    "    返り値:\n",
    "      out: ViTの出力。形状は(B, M)\n",
    "        B:バッチサイズ, M:クラス数\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    ## (B, C, H, W) --> (B, N, D)\n",
    "    ## N:トークン数(=バッチの数+1), D:ベクトルの長さ\n",
    "    out = self.input_layer(x)\n",
    "    # Encoder\n",
    "    ## (B, N, D) --> (B, N, D)\n",
    "    out = self.encoder(out)\n",
    "    # クラストークンのみ抜き出す\n",
    "    ## (B, N, D) - -> (B, D)\n",
    "    cls_token = out[:,0]\n",
    "    # MLP Head\n",
    "    ## (B, D) --> (B, M)\n",
    "    pred = self.mlp_head(cls_token)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_classes = 10\n",
    "batch_size, channel, height, width = 2, 3, 32, 32\n",
    "x = torch.randn(batch_size, channel, height, width)\n",
    "vit = Vit(in_channels=channel, num_classes=num_classes)\n",
    "pred = vit(x)\n",
    "\n",
    "# (2, 10)(=(B, M))になっていることを確認\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6ce19cd792589e8ccf8babb3986c3b6ee32845423f27865ddfac7006bec93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
