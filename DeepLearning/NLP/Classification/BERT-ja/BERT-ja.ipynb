{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.rondhuit.com/download.html#ldcc\n",
    "# wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "# tar -zxvf ldcc-20140209.tar.gz\n",
    "\n",
    "\n",
    "# 下記のプログラムでは、ニュースの各ファイルの本文を抽出し、改行、全角スペース、タブを除去して、1行の文に変換したのちに、カテゴリの番号（0-9）とのセットにします。\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "raw_data_path = \"./text\"  # ライブドアニュースを格納したディレクトリ\n",
    "\n",
    "dir_files = os.listdir(path=raw_data_path)\n",
    "dirs = [f for f in dir_files if os.path.isdir(os.path.join(raw_data_path, f))]\n",
    "text_label_data = []  # 文章とラベル（カテゴリ）のセット\n",
    "\n",
    "for i in range(len(dirs)):\n",
    "    dir = dirs[i]\n",
    "    files = glob.glob(os.path.join(raw_data_path, dir, \"*.txt\"))\n",
    "\n",
    "    for file in files:\n",
    "        if os.path.basename(file) == \"LICENSE.txt\": # 各ディレクトリにあるLICENSE.txtを除外する\n",
    "            continue\n",
    "\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.readlines()[3:]\n",
    "            text = \"\".join(text)\n",
    "            text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n",
    "            text_label_data.append([text, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用、テスト用データの作成、保存\n",
    "# 先ほど作成した、本文、ラベル（カテゴリ）のセットを、学習用、評価用のデータに分割し、CSVファイル（news_train.csv、news_test.csv）として保存します。\n",
    "\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news_train, news_test =  train_test_split(text_label_data, shuffle=True)  # データを学習用とテスト用に分割\n",
    "data_path = \"./data\"\n",
    "\n",
    "with open(os.path.join(data_path, \"news_train.csv\"), \"w\") as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerows(news_train) # \n",
    "\n",
    "with open(os.path.join(data_path, \"news_test.csv\"), \"w\") as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerows(news_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先ほど作成したデータを入力とし、BERTを使ってニュースのカテゴリ分類を学習（fine-tuning）させるプログラムtrain.pyを作成します。\n",
    "\n",
    "# モデル、トークナイザーの読み込み\n",
    "# transformersに含まれている文章を分類するためのモデルBertForSequenceClassification、日本語を形態素解析するためのトークナイザーBertJapaneseTokenizerを読み込みます。\n",
    "\n",
    "# cl-tohoku/bert-base-japanese-whole-word-maskingは事前学習済みの日本語BERTモデルです。\n",
    "# このモデルは、東北大学の乾研究室によって作成されたもので、こちらのページで公開されています。\n",
    "# https://www.nlp.ecei.tohoku.ac.jp/news-release/3284/\n",
    "\n",
    "# BertForSequenceClassification、BertJapaneseTokenizerを読み込んだ際に、自動的にダウンロードされるため、あらかじめダウンロードをする必要はありません。\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "\n",
    "# モデル\n",
    "model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=9)\n",
    "# model.cuda() # cudaを使う場合は、この行を有効にする\n",
    "# トークナイザー\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "学習用、テスト用データの読み込み\n",
    "\"\"\"\n",
    "# 学習の入力データとして、先ほど保存したデータを読み込みます。\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# トークナイズ用関数\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "data_path = \"./data\"\n",
    "\n",
    "# 学習用データ\n",
    "train_data = load_dataset(\"csv\", data_files=os.path.join(data_path, \"news_train.csv\"), column_names=[\"text\", \"label\"], split=\"train\")\n",
    "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
    "train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
    "\n",
    "# テスト用データ\n",
    "test_data = load_dataset(\"csv\", data_files=os.path.join(data_path, \"news_test.csv\"), column_names=[\"text\", \"label\"], split=\"train\")\n",
    "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
    "test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"]) \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Trainerの初期化\n",
    "\"\"\"\n",
    "# Trainerに、学習対象のモデル、学習用パラメーター、評価用関数、学習用データ、評価用データを設定して初期化します。\n",
    "\n",
    "# 評価用関数\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(result):\n",
    "    labels = result.label_ids\n",
    "    preds = result.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainerの設定\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 学習用パラメーター\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n",
    "    weight_decay = 0.01,  # 重みの減衰率\n",
    "    # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ありません\n",
    "    logging_dir = \"./logs\",\n",
    ")\n",
    "\n",
    "# Trainerの初期化\n",
    "trainer = Trainer(\n",
    "    model = model, # 学習対象のモデル\n",
    "    args = training_args, # 学習用パラメーター\n",
    "    compute_metrics = compute_metrics, # 評価用関数\n",
    "    train_dataset = train_data, # 学習用データ\n",
    "    eval_dataset = test_data, # テスト用データ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "# Trainerを使って、モデルの学習、評価を行います。\n",
    "trainer.train() # 学習\n",
    "trainer.evaluate() # 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルの保存\n",
    "# 後で使うために、学習済みモデル、トークナイザーを保存します。\n",
    "model_dir = \"./model\"\n",
    "trainer.save_model(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:04:25.346423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 22:04:26.058578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 22:04:26.058672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 22:04:26.058679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# では、学習したモデルを使って、ニュースが正しく分類できるかどうかを確認します。\n",
    "# 今回はsports-watchディレクトリにあるsports-watch-4764756.txtを入力として分類を行います。\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "\n",
    "# 学習済みモデルの読み込み\n",
    "model_dir = \"./model\"\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "# loaded_model.cuda() # cudaを使う場合は、この行を有効にする\n",
    "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# 分類するデータの読み込み\n",
    "file = \"./text/sports-watch/sports-watch-4764756.txt\"  # sports-watchの適当なニュース\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "  sample_text = f.readlines()[3:]\n",
    "  sample_text = \"\".join(sample_text)\n",
    "  sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "words = loaded_tokenizer.tokenize(sample_text)\n",
    "word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
    "word_tensor = torch.tensor([word_ids[:max_length]])  # Tensorに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['サッカー', 'W', '杯', '南アフリカ', '大会']\n",
      "341 語\n",
      "[1301, 472, 3411, 7716, 622]\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "print(words[:5])\n",
    "print(f\"{len(words)} 語\")\n",
    "print(word_ids[:5])\n",
    "print(len(word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "結果は sports-watch\n"
     ]
    }
   ],
   "source": [
    "# 予測の実行\n",
    "word_tensor.cuda()  # cudaを使う場合は、この行を有効にする\n",
    "y = loaded_model(word_tensor)  # 結果の予測\n",
    "pred = y[0].argmax(-1)  # 最大値のインデックス（ディレクトリの番号）\n",
    "\n",
    "# 結果の標準\n",
    "path = \"./text\"\n",
    "dir_files = os.listdir(path=path)\n",
    "dirs = [f for f in dir_files if os.path.isdir(os.path.join(path, f))]  # ディレクトリ一覧\n",
    "print(\"結果は\", dirs[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c2760a9c2997f9bc5632bfe1b04c6e39ec7256b7fa1e795474f5afcfd79ce3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
