{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://amaru-ai.com/entry/2022/10/15/202442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読込\n",
    "まずは対象のデータをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-19 16:35:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29224203 (28M) [application/x-httpd-php]\n",
      "Saving to: ‘NewsAggregatorDataset.zip’\n",
      "\n",
      "NewsAggregatorDatas 100%[===================>]  27.87M  6.64MB/s    in 4.2s    \n",
      "\n",
      "2022-12-19 16:35:41 (6.64 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
      "\n",
      "Archive:  NewsAggregatorDataset.zip\n",
      "  inflating: 2pageSessions.csv       \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._2pageSessions.csv  \n",
      "  inflating: newsCorpora.csv         \n",
      "  inflating: __MACOSX/._newsCorpora.csv  \n",
      "  inflating: readme.txt              \n",
      "  inflating: __MACOSX/._readme.txt   \n"
     ]
    }
   ],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "# !unzip NewsAggregatorDataset.zip\n",
    "# !mkdir -p ../../../../datasets/nlp/classification\n",
    "# !mv 2pageSessions.csv ../../../../datasets/nlp/classification/\n",
    "# !mv newsCorpora.csv ../../../../datasets/nlp/classification/\n",
    "# !mv readme.txt ../../../../datasets/nlp/classification/\n",
    "# !rm NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  422937 ../../../datasets/nlp/classification/newsCorpora.csv\n"
     ]
    }
   ],
   "source": [
    "# 行数の確認\n",
    "!wc -l ../../../../datasets/nlp/classification/newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tFed official says weak data caused by weather, should not slow taper\thttp://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\?track=rss\tLos Angeles Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.latimes.com\t1394470370698\n",
      "2\tFed's Charles Plosser sees high bar for change in pace of tapering\thttp://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html\tLivemint\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.livemint.com\t1394470371207\n",
      "3\tUS open: Stocks fall after Fed official hints at accelerated tapering\thttp://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371550\n",
      "4\tFed risks falling 'behind the curve', Charles Plosser says\thttp://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371793\n",
      "5\tFed's Plosser: Nasty Weather Has Curbed Job Growth\thttp://www.moneynews.com/Economy/federal-reserve-charles-plosser-weather-job-growth/2014/03/10/id/557011\tMoneynews\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.moneynews.com\t1394470372027\n"
     ]
    }
   ],
   "source": [
    "# 先頭10行の確認\n",
    "!head -5 ../../../../datasets/nlp/classification/newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "!sed -e 's/\"/'\\''/g' ../../../../datasets/nlp/classification/newsCorpora.csv > ../../../../datasets/nlp/classification/newsCorpora_re.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 続いて、データフレームとして読込み、情報源（PUBLISHER）がReuters, Huffington Post, Businessweek, Contactmusic.com, Daily Mailの事例のみを抽出した後、学習データ、検証データ、評価データに分割しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               TITLE CATEGORY\n",
      "0  REFILE-UPDATE 1-European car sales up for sixt...        b\n",
      "1  Amazon Plans to Fight FTC Over Mobile-App Purc...        t\n",
      "2  Kids Still Get Codeine In Emergency Rooms Desp...        m\n",
      "3  What On Earth Happened Between Solange And Jay...        e\n",
      "4  NATO Missile Defense Is Flight Tested Over Hawaii        b\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('../../../../datasets/nlp/classification/newsCorpora_re.csv',\n",
    "                 header=None,\n",
    "                 sep='\\t', \n",
    "                 names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n",
    "# データの分割\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "valid.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【学習データ】\n",
      "b    4501\n",
      "e    4235\n",
      "t    1220\n",
      "m     728\n",
      "Name: CATEGORY, dtype: int64\n",
      "【検証データ】\n",
      "b    563\n",
      "e    529\n",
      "t    153\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n",
      "【評価データ】\n",
      "b    563\n",
      "e    530\n",
      "t    152\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 事例数の確認\n",
    "# (b: ビジネス、e: エンターテインメント、t: 科学技術、m: 健康)\n",
    "print('【学習データ】')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('【検証データ】')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('【評価データ】')\n",
    "print(test['CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の準備\n",
    "\n",
    "BERTモデルを利用するためにtransformersライブラリをインストールしておきます。transformersを通じて、BERT以外にも多くの事前学習済みモデルを短いコードで非常に簡単に利用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次に、データをモデルに投入可能な形に整形します。\n",
    "\n",
    "# まずは、PyTorchでよく利用される、特徴ベクトルとラベルベクトルを合わせて保持するDatasetを作成するためのクラスを定義します。\n",
    "# このクラスにtokenizerを渡すことで、入力テキストの前処理を行い、指定した最長系列長までパディングした上で単語IDに変換する処理を実現できるようにしておきます。\n",
    "# とはいえ、BERT用にすべての処理が書かれたtokenizerそのものは、のちほどtranformersを通じて取得するため、クラス内で必要なものはtokenizerに渡す処理と結果を受け取る処理のみです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの定義\n",
    "class NewsDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):  # len(Dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
    "    text = self.X[index]\n",
    "    inputs = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      pad_to_max_length=True\n",
    "    )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'ids': torch.LongTensor(ids),\n",
    "      'mask': torch.LongTensor(mask),\n",
    "      'labels': torch.Tensor(self.y[index])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記を用いてDatasetを作成します。\n",
    "\n",
    "# なお、英語版事前学習済みモデルとして利用できるBERTは、最高精度を目指した構成であるLARGEと、それよりパラメータの少ないBASE、\n",
    "# それらのそれぞれについて小文字のみ（Uncased）と大文字小文字混在（Cased）の4パターンがあります。\n",
    "# 今回は、手軽に試すことができるBASEのUncasedを使っていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: tensor([  101, 25416,  9463,  1011, 10651,  1015,  1011,  2647,  2482,  4341,\n",
      "         2039,  2005,  4369,  3204,  2004, 18730,  8980,   102,     0,     0])\n",
      "mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "labels: tensor([1., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukik/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 正解ラベルのone-hot化\n",
    "y_train = pd.get_dummies(train, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
    "y_valid = pd.get_dummies(valid, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
    "y_test = pd.get_dummies(test, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
    "\n",
    "# Datasetの作成\n",
    "max_len = 20\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset_train = NewsDataset(train['TITLE'], y_train, tokenizer, max_len)\n",
    "dataset_valid = NewsDataset(valid['TITLE'], y_valid, tokenizer, max_len)\n",
    "dataset_test = NewsDataset(test['TITLE'], y_test, tokenizer, max_len)\n",
    "\n",
    "for var in dataset_train[0]:\n",
    "  print(f'{var}: {dataset_train[0][var]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1文目の文の情報を出力しています。\n",
    "\n",
    "# 入力文字列がidsとしてID系列に変換されていることが確認できます。\n",
    "# BERTでは、変換の過程で元の文の文頭と文末に特殊区切り文字である[CLS]と[SEP]がそれぞれ挿入されるため、それらも101と102として系列に含まれています。0はパディングを表します。\n",
    "# 正解ラベルもlabelsとしてone-hot形式で保持しています。\n",
    "\n",
    "# また、パディングの位置を表すmaskも合わせて保持し、学習時にidsと一緒にモデルに渡せるようにしておきます。\n",
    "\n",
    "# 続いて、ネットワークを定義します。 transfomersを用いることで、BERT部分はまるごとBertModelで表現可能です。\n",
    "# その後、分類タスクに対応するため、BERTの出力ベクトルを受け取るドロップアウトと全結合層を定義すれば完成です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT分類モデルの定義\n",
    "class BERTClass(torch.nn.Module):\n",
    "  def __init__(self, drop_rate, otuput_size):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.drop = torch.nn.Dropout(drop_rate)\n",
    "    self.fc = torch.nn.Linear(768, otuput_size)  # BERTの出力に合わせて768次元を指定\n",
    "    \n",
    "  def forward(self, ids, mask):\n",
    "    _, out = self.bert(ids, attention_mask=mask, return_dict=False)\n",
    "    out = self.fc(self.drop(out))\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT分類モデルの学習\n",
    "\n",
    "ここまでで、Datasetとネットワークが準備できたため、あとは普段通りの学習ループを作成します。 ここでは一連の流れをtrain_model関数として定義しています。\n",
    "\n",
    "登場する構成要素の意味については、[【言語処理100本ノック 2020】第8章: ニューラルネット](https://amaru-ai.com/entry/2022/10/14/204710)の記事の中で、問題の流れに沿って解説していますので、そちらをご参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
    "  \"\"\" 損失・正解率を計算\"\"\"\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # デバイスの指定\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # 順伝播\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      # 損失計算\n",
    "      loss += criterion(outputs, labels).item()\n",
    "\n",
    "      # 正解率計算\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # バッチサイズの長さの予測ラベル配列\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # バッチサイズの長さの正解ラベル配列\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "      \n",
    "  return loss / len(loader), correct / total\n",
    "  \n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
    "  # デバイスの指定\n",
    "  model.to(device)\n",
    "\n",
    "  # dataloaderの作成\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # 学習\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 開始時刻の記録\n",
    "    s_time = time.time()\n",
    "\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    for data in dataloader_train:\n",
    "      # デバイスの指定\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # 勾配をゼロで初期化\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "      outputs = model(ids, mask)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # チェックポイントの保存\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # 終了時刻の記録\n",
    "    e_time = time.time()\n",
    "\n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータを設定して、ファインチューニングを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m cuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m# モデルの学習\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m log \u001b[39m=\u001b[39m train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device\u001b[39m=\u001b[39;49mdevice)\n",
      "Cell \u001b[0;32mIn[16], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m   loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     59\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 60\u001b[0m   optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     62\u001b[0m \u001b[39m# 損失と正解率の算出\u001b[39;00m\n\u001b[1;32m     63\u001b[0m loss_train, acc_train \u001b[39m=\u001b[39m calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
      "File \u001b[0;32m~/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Work/ML/ML-Library/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:318\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 318\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# パラメータの設定\n",
    "DROP_RATE = 0.4\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# モデルの定義\n",
    "model = BERTClass(DROP_RATE, OUTPUT_SIZE)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# デバイスの指定\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# モデルの学習\n",
    "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログの可視化\n",
    "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
    "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
    "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解率の算出\n",
    "def calculate_accuracy(model, dataset, device):\n",
    "  # Dataloaderの作成\n",
    "  loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "  model.eval()\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      # デバイスの指定\n",
    "      ids = data['ids'].to(device)\n",
    "      mask = data['mask'].to(device)\n",
    "      labels = data['labels'].to(device)\n",
    "\n",
    "      # 順伝播 + 予測値の取得 + 正解数のカウント\n",
    "      outputs = model.forward(ids, mask)\n",
    "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
    "      labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
    "      total += len(labels)\n",
    "      correct += (pred == labels).sum().item()\n",
    "\n",
    "  return correct / total\n",
    "\n",
    "print(f'正解率（学習データ）：{calculate_accuracy(model, dataset_train, device):.3f}')\n",
    "print(f'正解率（検証データ）：{calculate_accuracy(model, dataset_valid, device):.3f}')\n",
    "print(f'正解率（評価データ）：{calculate_accuracy(model, dataset_test, device):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6ce19cd792589e8ccf8babb3986c3b6ee32845423f27865ddfac7006bec93a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
